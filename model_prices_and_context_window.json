{
  "sample_spec": {
        "max_tokens": "set to max_output_tokens if provider specifies it. IF not set to max_tokens provider specifies", 
        "max_input_tokens": "max input tokens, if the provider specifies it. if not default to max_tokens",
        "max_output_tokens": "max output tokens, if the provider specifies it. if not default to max_tokens", 
        "input_cost_per_token": 0.0000,
        "output_cost_per_token": 0.000,
        "litellm_provider": "one of https://docs.litellm.ai/docs/providers",
        "mode": "one of chat, embedding, completion, image_generation, audio_transcription, audio_speech",
        "supports_function_calling": true,
        "supports_parallel_function_calling": true,
        "supports_vision": true,
        "supports_audio": true,
        "supports_prompt_caching": true
    },
    "meta-llama/CodeLlama-13b-Instruct-hf": {
        "input_cost_per_token": 0.0000005,
        "output_cost_per_token": 0.00000072,
        "litellm_provider": "ionos",
        "mode": "chat",
        "supports_function_calling": true,
        "supports_prompt_caching": true
    },
    "meta-llama/Meta-Llama-3.1-405B-Instruct-FP8": {
        "input_cost_per_token": 0.00000165,
        "output_cost_per_token": 0.00000193,
        "litellm_provider": "ionos",
        "mode": "chat",
        "supports_function_calling": true,
        "supports_prompt_caching": true
    },
    "meta-llama/Meta-Llama-3.1-70B-Instruct": {
        "max_tokens": 4097,
        "max_input_tokens": 16385,
        "max_output_tokens": 4096,
        "input_cost_per_token": 0.00000165,
        "output_cost_per_token": 0.00000193,
        "litellm_provider": "ionos",
        "mode": "chat",
        "supports_function_calling": true,
        "supports_prompt_caching": true
    },
    "meta-llama/Meta-Llama-3.1-8B-Instruct": {
        "input_cost_per_token": 0.00000017,
        "output_cost_per_token": 0.00000028,
        "litellm_provider": "ionos",
        "mode": "chat",
        "supports_function_calling": true,
        "supports_prompt_caching": true
    },
      "mistralai/Mistral-7B-Instruct-v0.3": {
        "input_cost_per_token": 0.00000017,
        "output_cost_per_token": 0.00000028,
        "litellm_provider": "ionos",
        "mode": "chat",
        "supports_function_calling": true,
        "supports_prompt_caching": true
    },
      "mistralai/Mixtral-8x7B-Instruct-v0.1": {
        "input_cost_per_token": 0.0000005,
        "output_cost_per_token": 0.00000072,
        "litellm_provider": "ionos",
        "mode": "chat",
        "supports_function_calling": true,
        "supports_prompt_caching": true
    },
}
